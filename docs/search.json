[
  {
    "objectID": "workbook_10.html",
    "href": "workbook_10.html",
    "title": "Introduction to quantitative time diary analysis",
    "section": "",
    "text": "R studio has many functionalities, but for the purpose of this workshop, we will keep it simple and use it merely as a two-windows text editor.\nWe first need to load the R libraries we are going to need:\n- dplyr for data manipulation\n- haven for opening stata/SPSS datasets\n- ggplot2 for advanced graphic functions\n\nrm(list=ls()) ### deleting any existing objects\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(haven)\n\nThe next thing is to set up the working directory, and open the dataset. We could also just specify the absolute file paths in each file opening ie read_dta() command every time. The line below works on my computer, but needs to be changed accordingly on your system depending where your files are stored )\n\n#setwd(\"C://Users/qtnvpw1/Dropbox/work/UKDS/trainings/TU2023/Day_1/practical\")\nsetwd(\"~/Dropbox/work/UKDS/trainings/TU2023/Day_1/practical\")\n\nFor this demonstration, we will be using a subsample of the Multinational Time Use Study - MTUS. We will open the Stata dataset using read_dta() from the haven package, which retains both the original numeric values and the Stata’s variable and value labels.\nWe begin by removing from the sample a few variables that we will not be using, and the children respondents to keep the dataset as simple as possible:\n\nep&lt;-data.frame(\n    read_dta(\"data/mtus_teach_ep.dta\") %&gt;%\n    select(-msamp,-swave,-ict,-core25,-inout,-diary) %&gt;%\n    filter(child!=1)\n)"
  },
  {
    "objectID": "workbook_10.html#setting-things-up",
    "href": "workbook_10.html#setting-things-up",
    "title": "Introduction to quantitative time diary analysis",
    "section": "",
    "text": "R studio has many functionalities, but for the purpose of this workshop, we will keep it simple and use it merely as a two-windows text editor.\nWe first need to load the R libraries we are going to need:\n- dplyr for data manipulation\n- haven for opening stata/SPSS datasets\n- ggplot2 for advanced graphic functions\n\nrm(list=ls()) ### deleting any existing objects\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(haven)\n\nThe next thing is to set up the working directory, and open the dataset. We could also just specify the absolute file paths in each file opening ie read_dta() command every time. The line below works on my computer, but needs to be changed accordingly on your system depending where your files are stored )\n\n#setwd(\"C://Users/qtnvpw1/Dropbox/work/UKDS/trainings/TU2023/Day_1/practical\")\nsetwd(\"~/Dropbox/work/UKDS/trainings/TU2023/Day_1/practical\")\n\nFor this demonstration, we will be using a subsample of the Multinational Time Use Study - MTUS. We will open the Stata dataset using read_dta() from the haven package, which retains both the original numeric values and the Stata’s variable and value labels.\nWe begin by removing from the sample a few variables that we will not be using, and the children respondents to keep the dataset as simple as possible:\n\nep&lt;-data.frame(\n    read_dta(\"data/mtus_teach_ep.dta\") %&gt;%\n    select(-msamp,-swave,-ict,-core25,-inout,-diary) %&gt;%\n    filter(child!=1)\n)"
  },
  {
    "objectID": "workbook_10.html#inspecting-the-data",
    "href": "workbook_10.html#inspecting-the-data",
    "title": "Introduction to quantitative time diary analysis",
    "section": "2. Inspecting the data",
    "text": "2. Inspecting the data\nLets have a look at the data.\n\ndim(ep)\n\n[1] 1811879      20\n\n\nThere are many observations and few variables: this is typical of an episode file in long format.\nLet’s inspect the variables in the dataset, sequentially…\n\nnames(ep)\n\n [1] \"country\" \"survey\"  \"hldid\"   \"persid\"  \"id\"      \"day\"     \"year\"   \n [8] \"time\"    \"clockst\" \"start\"   \"end\"     \"epnum\"   \"main\"    \"sec\"    \n[15] \"eloc\"    \"mtrav\"   \"alone\"   \"child\"   \"sppart\"  \"oad\"    \n\n\n… Or alphabetically\n\nls(ep)\n\n [1] \"alone\"   \"child\"   \"clockst\" \"country\" \"day\"     \"eloc\"    \"end\"    \n [8] \"epnum\"   \"hldid\"   \"id\"      \"main\"    \"mtrav\"   \"oad\"     \"persid\" \n[15] \"sec\"     \"sppart\"  \"start\"   \"survey\"  \"time\"    \"year\"   \n\n\nLet’s take a first look at the data. We can visualise the first few lines of raw data, without the Stata value labels. More information on the variable deifinition is available in the MTUS documentation\n\nhead(ep)\n\n  country survey hldid persid id day year time clockst start end epnum main sec\n1      ES   2009     1      1  1   1 2010  240     6.0     0 240     1    2  69\n2      ES   2009     1      1  1   1 2010   20    10.0   240 260     2    6  69\n3      ES   2009     1      1  1   1 2010   10    10.2   260 270     3    4  69\n4      ES   2009     1      1  1   1 2010   30    10.3   270 300     4    4  69\n5      ES   2009     1      1  1   1 2010   30    11.0   300 330     5   20  69\n6      ES   2009     1      1  1   1 2010   90    11.3   330 420     6   43  62\n  eloc mtrav alone child sppart oad\n1    2    -7     0     0      0   0\n2    2    -7     0     0      1   0\n3    2    -7     1     0      0   0\n4    2    -7     1     0      0   0\n5    2    -7     1     0      0   0\n6    9    -7     0     0      1   0\n\n\nWe can have a closer look at the labelled values for the Main (ie primary) activity variable.\n\ntable(ep$main)\n\n\n     1      2      3      4      5      6      7      8      9     10     11 \n   396 209307     10 190292   9358 212870  62246  10724    829     29   3243 \n    12     13     14     15     16     17     18     19     20     21     22 \n  5123   4750    995   8772   8812   2606  96053  52750  51951  29530  21144 \n    23     24     25     26     27     28     29     30     31     32     33 \n 31673  41551   5192   4515  10120  13098    904   4574   1968   8703  18467 \n    34     35     36     37     38     39     40     41     42     43     44 \n  6701   1789   1148   2101   1424  10065   2896    292  12626  17843    415 \n    45     46     47     48     49     50     51     52     53     54     55 \n   108  12322  10615  46916  23909  11637    378   3605   1667   4194  36461 \n    56     57     58     59     60     61     62     63     64     65     66 \n 40863   2011  12149 148394   5689  29548    212  50738  10104   9686  15880 \n    67     68     69 \n 58982  82367  13589 \n\nhead(as_factor(ep$main)) ### This would be a very long output!\n\n[1] sleep and naps                  meals or snacks in other places\n[3] wash, dress, care for self      wash, dress, care for self     \n[5] cleaning                        walking                        \n69 Levels: imputed personal or household care sleep and naps ... no recorded activity\n\n\nThe only (small) downside of the haven package is that it does not store the data into ‘pure’ data frames:\n\nclass(ep)\n\n[1] \"data.frame\"\n\nclass(ep$main)\n\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"        \n\n\nWe can then explore a few variables of interest, for example the number of episodes by country\n\ntable(ep$country)\n\n\n    ES     FR     NL     UK     US \n395722 468889 286329 476199 184740 \n\ntable(as_factor(ep$country))\n\n\n    ES     NL     FR     UK     US \n395722 286329 468889 476199 184740 \n\n\n… by survey year…\n\ntable(ep$survey)\n\n\n  2000   2009   2012   2014 \n286329 864611 184740 476199 \n\n\nNote that the year the survey was conducted (SURVEY) – may be different from the year the diary was filled in (YEAR):\n\nxtabs(~country+year,ep)\n\n       year\ncountry   2000   2009   2010   2012   2014   2015\n     ES      0  90512 305210      0      0      0\n     FR      0      0 468889      0      0      0\n     NL 286329      0      0      0      0      0\n     UK      0      0      0      0 283187 193012\n     US      0      0      0 184740      0      0\n\n\nIn order to avoid confusion, let’s create a unique STUDY variable\n\nep$study&lt;-paste(ep$country,ep$survey,sep=\" \")\n\nxtabs(~study,ep)\n\nstudy\nES 2009 FR 2009 NL 2000 UK 2014 US 2012 \n 395722  468889  286329  476199  184740 \n\n\nWe can also explore the distribution of days by retaining only one episode per day\n\nxtabs(~id+study, subset=epnum==1,ep)\n\n   study\nid  ES 2009 FR 2009 NL 2000 UK 2014 US 2012\n  1   19253   16195    1813    7673   12373\n  2       0   11630    1813    7666       0\n  3       0       0    1813       0       0\n  4       0       0    1813       0       0\n  5       0       0    1813       0       0\n  6       0       0    1813       0       0\n  7       0       0    1813       0       0\n\n\nAnd examine it as a proper contingency table, with column percentages\n\n100*prop.table(\n  xtabs(~id+study, subset=epnum==1,ep)\n  ,2)\n\n   study\nid    ES 2009   FR 2009   NL 2000   UK 2014   US 2012\n  1 100.00000  58.20305  14.28571  50.02282 100.00000\n  2   0.00000  41.79695  14.28571  49.97718   0.00000\n  3   0.00000   0.00000  14.28571   0.00000   0.00000\n  4   0.00000   0.00000  14.28571   0.00000   0.00000\n  5   0.00000   0.00000  14.28571   0.00000   0.00000\n  6   0.00000   0.00000  14.28571   0.00000   0.00000\n  7   0.00000   0.00000  14.28571   0.00000   0.00000\n\nround(\n  100*prop.table(\n    xtabs(~id+study, subset=epnum==1,ep)\n    ,2)\n  ,1)\n\n   study\nid  ES 2009 FR 2009 NL 2000 UK 2014 US 2012\n  1   100.0    58.2    14.3    50.0   100.0\n  2     0.0    41.8    14.3    50.0     0.0\n  3     0.0     0.0    14.3     0.0     0.0\n  4     0.0     0.0    14.3     0.0     0.0\n  5     0.0     0.0    14.3     0.0     0.0\n  6     0.0     0.0    14.3     0.0     0.0\n  7     0.0     0.0    14.3     0.0     0.0\n\n\nLet us now examine the structure of the data\nWe have people identified with PERSID in households HLDID with sequential diary number ID… Within studies\n\nep$main.f&lt;-as_factor(ep$main)\nep$eloc.f&lt;-as_factor(ep$eloc)\nep$alone.f&lt;-as_factor(ep$alone)\n\nhead(\n     ep |&gt; \n          select(study, hldid,persid,id,time,epnum,main.f,eloc.f,alone.f)\n)\n\n    study hldid persid id time epnum                          main.f\n1 ES 2009     1      1  1  240     1                  sleep and naps\n2 ES 2009     1      1  1   20     2 meals or snacks in other places\n3 ES 2009     1      1  1   10     3      wash, dress, care for self\n4 ES 2009     1      1  1   30     4      wash, dress, care for self\n5 ES 2009     1      1  1   30     5                        cleaning\n6 ES 2009     1      1  1   90     6                         walking\n             eloc.f                    alone.f\n1 at another?s home    others reported present\n2 at another?s home    others reported present\n3 at another?s home no others reported present\n4 at another?s home no others reported present\n5 at another?s home no others reported present\n6   other locations    others reported present\n\nprint(\n     (ep |&gt; \n          select(time,epnum,main.f,eloc.f,alone.f))[1:20,]\n)\n\n   time epnum                              main.f            eloc.f\n1   240     1                      sleep and naps at another?s home\n2    20     2     meals or snacks in other places at another?s home\n3    10     3          wash, dress, care for self at another?s home\n4    30     4          wash, dress, care for self at another?s home\n5    30     5                            cleaning at another?s home\n6    90     6                             walking   other locations\n7    30     7                        other travel        travelling\n8    30     8           food preparation, cooking at another?s home\n9    30     9     meals or snacks in other places at another?s home\n10   30    10 watch TV, video, DVD, streamed film at another?s home\n11   60    11                      sleep and naps at another?s home\n12  120    12 watch TV, video, DVD, streamed film at another?s home\n13   10    13 watch TV, video, DVD, streamed film at another?s home\n14  140    14                             walking   other locations\n15   10    15                        other travel        travelling\n16   40    16           food preparation, cooking at another?s home\n17   50    17     meals or snacks in other places at another?s home\n18  100    18 watch TV, video, DVD, streamed film at another?s home\n19  370    19                      sleep and naps at another?s home\n20  240     1                      sleep and naps at another?s home\n                      alone.f\n1     others reported present\n2     others reported present\n3  no others reported present\n4  no others reported present\n5  no others reported present\n6     others reported present\n7     others reported present\n8     others reported present\n9     others reported present\n10    others reported present\n11    others reported present\n12    others reported present\n13    others reported present\n14    others reported present\n15    others reported present\n16    others reported present\n17    others reported present\n18    others reported present\n19    others reported present\n20    others reported present"
  },
  {
    "objectID": "workbook_10.html#estimating-durations",
    "href": "workbook_10.html#estimating-durations",
    "title": "Introduction to quantitative time diary analysis",
    "section": "3. Estimating durations",
    "text": "3. Estimating durations\nLet’s compute the daily amount of time spent in paid work\nWe need to flag work episodes. We will be using a broad definition: work is any of the work-related tasks described below carried out either at home or outside home (but not commute).\nThe relevant MTUS activity codes are\n7 --  paid work-main job (not at home)                             \n8 --  paid work at home                 \n9 --  second or other job not at home       \n10 -- unpaid work to generate household income \n11 -- travel as a part of work\n12 -- work breaks\n13 -- other time at workplace\nThe first thing to do consist in flagging work in the episode dataset. One could use RECODE or GENERATE in Stata.\nIn R, I will use the ifelse() function (case_when() from the dplyr package would also work)\nLet’s have WK.T record the duration of any work related episode.\nIt takes the value 0 if no episode were recorded.\nThis is the simplest way of doing it in Base R\n\nep$wk.t&lt;-ifelse(ep$main&gt;=7 & ep$main&lt;=13,ep$time,0)\n\nAnother way which may be more parsimonious when recoding several variables simultaneously:\n\n  ep&lt;-ep%&gt;%mutate(wk.t=ifelse(main&gt;=7 & main&lt;=13,time,0))\n\nLet’s have a look at the variable\n\nsummary(ep$wk.t)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   0.000    0.000    0.000    6.296    0.000 1340.000 \n\nsummary(ep$wk.t[ep$wk.t&gt;0])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0    20.0    90.0   131.2   210.0  1340.0 \n\n\nThe mean seems rather low\n… In order to produce valid estimates, we need to first compute the amount of time spent on paid work … for each day of each respondent\n\nep&lt;-ep%&gt;%group_by(study,hldid,persid,id)%&gt;%\n    mutate(wk.b=sum(wk.t))%&gt;%ungroup()\n\nWe can now produce our first set of estimates and store it in an ad hoc data frame…\n\nres&lt;-ep%&gt;%filter(epnum==1)%&gt;%group_by(study)%&gt;%summarise(All=mean(wk.b))\n\nWe can even plot the results easily\n\nbarplot(res$All,names.arg=res$study, \n        main =\"Daily working time in selected countries\",\n        xlab=\"MTUS survey\",\n        ylab=\"Average daily minutes\")\n\n\n\n\nThe daily number of minutes in paid work seem a bit low. Could this be due to the fact that we do not differentiate between weekend and weekdays?\n\nep$wkd&lt;-ifelse(ep$day&gt;1 & ep$day&lt;7,\"Weekday\",\"Weekend\")\n\nLet’s add these to our results:\n\nres&lt;-cbind(res, \n           ep%&gt;%filter(epnum==1 & wkd==\"Weekday\")%&gt;%\n                group_by(study)%&gt;%\n                summarise(Weekday=mean(wk.b))%&gt;%select(Weekday),\n           ep%&gt;%filter(epnum==1& wkd==\"Weekend\")%&gt;%\n                group_by(study)%&gt;%\n                summarise(Weekend=mean(wk.b))%&gt;%\n                select(Weekend)\n           )\n\nFor technical reasons, in case of multiples bars, barplot() deals better with categories as row names rather than as a variable\n\nrownames(res)&lt;-res$study\nres&lt;-res%&gt;%select(-study)\n\n\nbarplot(t(as.matrix((res))),beside=T,ylim=c(0,350),\n        main =\"Daily working time in selected countries\",\n        xlab=\"MTUS survey\",\n        ylab=\"Average daily minutes\",\n        legend.text = c(\"Any day\", \"Weekday\", \"Weekend\"),\n        args.legend=list(x=\"top\",ncol=3)\n        )\n\n\n\n\nAs with most time use variables, we need to decide whether we are interested in an overall mean, which takes into account both people who did and did not engage in an activity, or instead a mean that reflects the typical daily working time of those who did work on the day\nLet’s repeat the exercise, this time with only those respondents with at least one minute of reported paid work aka ‘Participants’\n\nres.w&lt;-cbind(ep%&gt;%filter(epnum==1 & wk.b&gt;0)%&gt;%\n                  group_by(study)%&gt;%summarise(Part=mean(wk.b)),\n             ep%&gt;%filter(epnum==1 & wkd==\"Weekday\"  & wk.b&gt;0)%&gt;%\n                  group_by(study)%&gt;%\n                  summarise(Weekday=mean(wk.b))%&gt;%\n                  select(Weekday),\n            ep%&gt;%filter(epnum==1 & wkd==\"Weekend\"  & wk.b&gt;0)%&gt;%\n              group_by(study)%&gt;%\n              summarise(Weekend=mean(wk.b))%&gt;%\n              select(Weekend)\n)\nrownames(res.w)&lt;-res.w$study\nres.w&lt;-res.w%&gt;%select(-study)\n\n\nbarplot(t(as.matrix((res.w))),beside=T,ylim=c(0,700),\n        main =\"Daily working time in selected countries (participants)\",\n        xlab=\"MTUS survey\",\n        ylab=\"Average daily minutes\",\n        legend.text = c(\"Any day\", \"Weekday\", \"Weekend\"),\n        args.legend=list(x=\"top\",ncol=3)\n        )\n\n\n\n\nThese durationas are more realistic, but we need to keep in mind that the samples will differ between estimates. Respondent reporting paid work on diary day may not be exactly the same as those reporting doing shopping (more on this next week)"
  },
  {
    "objectID": "workbook_10.html#probability-of-engaging-in-paid-work",
    "href": "workbook_10.html#probability-of-engaging-in-paid-work",
    "title": "Introduction to quantitative time diary analysis",
    "section": "4. Probability of engaging in paid work",
    "text": "4. Probability of engaging in paid work\nFrom this, we can easily compute the probability of engaging in paid work on diary day\n\nres.p&lt;-cbind(\nep%&gt;%filter(epnum==1)%&gt;%group_by(study)%&gt;%summarise(p.all=mean(wk.b&gt;0)), \n\nep%&gt;%filter(epnum==1  & wkd==\"Weekday\")%&gt;%\n     group_by(study)%&gt;%\n     summarise(p.we=mean(wk.b&gt;0))%&gt;%select(p.we),\n\nep%&gt;%filter(epnum==1 & wkd==\"Weekend\")%&gt;%\n     group_by(study)%&gt;%\n     summarise(p.wk=mean(wk.b&gt;0))%&gt;%\n      select(p.wk) \n)\n\n\nrownames(res.p)&lt;-res.p$study\nres.p&lt;-res.p%&gt;%select(-study)\n\nbarplot(t(as.matrix(res.p)),beside=T,ylim=c(0,.7),\n        main =\"Daily probability of reporting paid work in selected countries\",\n        xlab=\"MTUS survey\",\n        ylab=\"Average daily probability\",\n        legend.text = c(\"Any day\", \"Weekday\", \"Weekend\"),\n        args.legend=list(x=\"top\",ncol=3)\n)"
  },
  {
    "objectID": "workbook_10.html#producing-grouped-estimates-by-individual-characteristics",
    "href": "workbook_10.html#producing-grouped-estimates-by-individual-characteristics",
    "title": "Introduction to quantitative time diary analysis",
    "section": "5. Producing grouped estimates by individual characteristics",
    "text": "5. Producing grouped estimates by individual characteristics\nWe first need to load the aggregate (ie day-level) file.\n\nd&lt;-read_dta(\"data/mtus_teach_ind.dta\")\n\nd$study&lt;-paste(d$country,d$survey,sep=\" \")\n\ndim(d)\n\n[1] 88865   142\n\n\nWe can then add the paid work estimates that we computed earlier. We discard observations not matching.\n\ndt&lt;-merge(d,\n          ep%&gt;%filter(epnum==1)%&gt;%select(study,hldid,persid,id,wkd,wk.b),\n          by=c(\"study\", \"hldid\",\"persid\",\"id\"),\n          all.x=F,all.y=F)\n\ndim(dt)\n\n[1] 87481   144\n\n\nLet’s create a more explicit gender variable\n\ndt$gender&lt;-as_factor(dt$sex)\nlevels(dt$gender)&lt;-c(\"Male\",\"Female\")\n\nWe can produce working-time estimates in one go this time, as we will be plotting the results with ggplot, which is able to exploit directly raw estimation results.\n\nres.g&lt;-dt%&gt;%filter(wk.b&gt;0 & age&gt;=16 & age&lt;=65)%&gt;%\n            group_by(study,gender,wkd)%&gt;%\n            summarise(Part=mean(wk.b))\n\nggplot(data=res.g, aes(x=study, y=Part,fill=gender)) +\n  geom_bar(stat=\"identity\", width=0.5,position = position_dodge())+\n  scale_fill_manual(values=c(\"#702082\",  \"#729fcf\",\"#00A9CE\"))+\n  coord_flip()+\n  facet_wrap(~wkd) +\n  labs(fill = \"Gender\", x=\"MTUS study\", y=\"Daily minutes of paid works\")+\n  theme_light()\n\n\n\n\nWe can follow the same logic to produce a plot of daily percentages of respondents engaging in paid work by gender.\n\nres.pg&lt;-dt%&gt;%filter(age&gt;=16 & age&lt;=65)%&gt;%\n            group_by(study,gender,wkd)%&gt;%\n            summarise(Part=round(100*mean(wk.b&gt;0),1))\n\nggplot(data=res.pg, aes(x=study, y=Part,fill=gender)) +\n  geom_bar(stat=\"identity\", width=0.5,position = position_dodge())+\n scale_fill_manual(values=c(\"#702082\",  \"#729fcf\",\"#00A9CE\"))+\n  coord_flip()+\n  facet_wrap(~wkd) +\n  labs(fill = \"Gender\", x=\"MTUS study\", y=\"Percent\")+\n  theme_bw()"
  }
]